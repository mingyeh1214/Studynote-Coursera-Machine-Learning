---
title: "Programming Exercise 1: Linear Regression"
output:
  html_document:
    toc: true
    number_sections: false
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
library(reticulate)
```

```{python}
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
```

# Python

## 2 Linear regression with one variable

```{python}
data = np.loadtxt(os.path.join('data', 'ex1data1.txt'), delimiter=',')
X, y = data[:, 0], data[:, 1]
m = y.size


def plotData(x, y):
    fig = plt.figure()  # open a new figure
    plt.plot(x, y, 'ro', ms=10, mec='k')
    plt.ylabel('Profit in $10,000')
    plt.xlabel('Population of City in 10,000s')

plotData(X, y)
plt.show()
```

### 2.1 Plotting the Data

### 2.2 Gradient Descent

#### 2.2.1 Update Equations

Hypothesis\
$$
h_{\theta}(x^{(i)})
=\theta_0\cdot1+\theta_1x^{(i)}
=\theta_0x_0^{(i)}+\theta_1x_1^{(i)}
=x^{(i)}\theta^T
$$
Cost Function\
這裡除以2是方便後面偏微分對消指數\
$$
J\left(\theta\right)
=J\left(\theta_0,\theta_1\right)
=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {{h}_{\theta}}\left({{x}^{(i)}}\right)-{{y}^{(i)}}\right)}^{2}}}
$$
Gradient Descent
$$
\theta_j
:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_j)
:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}\left((h_{\theta}(x^{(i)})-y^{(i)})\right)\cdot x_j^{(i)}
$$

#### 2.2.2 Implementation

```{python}
X = np.stack([np.ones(m), X], axis=1)
```

#### 2.2.3 Computing the cost $J(\theta)$

```{python}
def computeCost(X, y, theta):
    # X是m x 2矩陣,theta是1 x 2矩陣,y是m x 1矩陣
    m = y.size
    J = 0
    h = np.dot(X, theta)
    J = (1/(2 * m)) * np.sum(np.square(h - y))
    return J
```


```{python}
J = computeCost(X, y, theta=np.array([0.0, 0.0]))
print('With theta = [0, 0] \nCost computed = %.2f' % J)
```

```{python}
def gradientDescent(X, y, theta, alpha, num_iters):
  
    m = y.shape[0]
    
    theta = theta.copy()
    
    J_history = []
    
    for i in range(num_iters):
      
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
        
        J_history.append(computeCost(X, y, theta))
        
    return theta, J_history
```

```{python}
theta = np.zeros(2)

# some gradient descent settings
iterations = 1500
alpha = 0.01

theta, J_history = gradientDescent(X ,y, theta, alpha, iterations)
print('Theta found by gradient descent: {:.4f}, {:.4f}'.format(*theta))

```


```{python}
plotData(X[:, 1], y)
plt.plot(X[:, 1], np.dot(X, theta), '-')
plt.legend(['Training data', 'Linear regression']);
plt.show()
```

```{python}
predict1 = np.dot([1, 3.5], theta)
print('For population = 35,000, we predict a profit of {:.2f}\n'.format(predict1*10000))
```


```{python}
# grid over which we will calculate J
theta0_vals = np.linspace(-10, 10, 100)
theta1_vals = np.linspace(-1, 4, 100)

# initialize J_vals to a matrix of 0's
J_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))

# Fill out J_vals
for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        J_vals[i, j] = computeCost(X, y, [theta0, theta1])
        
# Because of the way meshgrids work in the surf command, we need to
# transpose J_vals before calling surf, or else the axes will be flipped
J_vals = J_vals.T

# surface plot
fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(121, projection='3d')
ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')
plt.xlabel('theta0')
plt.ylabel('theta1')
plt.title('Surface')

# contour plot
# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100
ax = plt.subplot(122)
plt.contour(theta0_vals, theta1_vals, J_vals, linewidths=2, cmap='viridis', levels=np.logspace(-2, 3, 20))
plt.xlabel('theta0')
plt.ylabel('theta1')
plt.plot(theta[0], theta[1], 'ro', ms=10, lw=2)
plt.title('Contour, showing minimum')
plt.show()

```



```{python}
data = np.loadtxt(os.path.join('data', 'ex1data2.txt'), delimiter=',')
X = data[:, :2]
y = data[:, 2]
m = y.size
```

```{python}
def featureNormalize(X):
    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma
    return X_norm, mu, sigma
```

```{python}
X_norm, mu, sigma = featureNormalize(X)

print('Computed mean:', mu)
print('Computed standard deviation:', sigma)
```


```{python}
X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)
```

```{python}
def computeCostMulti(X, y, theta):
    m = y.shape[0] # number of training examples
    J = 0
    h = np.dot(X, theta)
    J = (1/(2 * m)) * np.sum(np.square(h - y))
    return J
```

```{python}
def gradientDescentMulti(X, y, theta, alpha, num_iters):
    m = y.shape[0] # number of training examples
    theta = theta.copy()
    J_history = []
    
    for i in range(num_iters):
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
        J_history.append(computeCostMulti(X, y, theta))
    
    return theta, J_history
```

```{python}
# Choose some alpha value - change this
alpha = 0.1
num_iters = 400

# init theta and run gradient descent
theta = np.zeros(3)
theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)

# Plot the convergence graph
fig = plt.figure(figsize=(12, 5))
plt.plot(np.arange(len(J_history)), J_history, lw=2)
plt.xlabel('Number of iterations')
plt.ylabel('Cost J')
plt.show()

# Display the gradient descent's result
print('theta computed from gradient descent: {:s}'.format(str(theta)))

# Estimate the price of a 1650 sq-ft, 3 br house
# ======================= YOUR CODE HERE ===========================
# Recall that the first column of X is all-ones. 
# Thus, it does not need to be normalized.

X_array = [1, 1650, 3]
X_array[1:3] = (X_array[1:3] - mu) / sigma
price = np.dot(X_array, theta)   # You should change this

# ===================================================================

print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${:.0f}'.format(price))
```

