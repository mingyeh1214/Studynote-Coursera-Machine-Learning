---
title: "機器學習練習1-線性迴歸"
output:
  html_document:
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
library(reticulate)
```

# 單變量線性迴歸
```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

```{python}
path = 'ex1data1.txt'
data = pd.read_csv(path, header=None, names=['Population', 'Profit'])
data.head()
data.describe()
data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))
plt.show()
```

Cost Function\
這裡除以2是方便後面偏微分對消指數\
$$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}} \right)}^{2}}}$$
其中：
$$
{{h}_{\theta }}\left( x \right)={{\theta }^{T}}X={{\theta }_{0}}{{x}_{0}}+{{\theta }_{1}}{{x}_{1}}+{{\theta }_{2}}{{x}_{2}}+...+{{\theta }_{n}}{{x}_{n}}
$$

```{python}
def computeCost(X, y, theta):
    # X是m x 2矩陣,theta是1 x 2矩陣,y是m x 1矩陣
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
```

```{python}
data.insert(loc=0, column='Ones', value=1)
data.head()
```

```{python}
# set X (training data) and y (target variable)
cols = data.shape[1]
X = data.iloc[:,0:cols-1]# X是所有行，去掉最後一列
y = data.iloc[:,cols-1:cols]# y是所有行，最後一列
X.head()
y.head()
```

```{python}
# 資料轉換array -> matrix
X = np.matrix(X.values)
y = np.matrix(y.values)
theta = np.matrix(np.array([0,0]))
X.shape, y.shape, theta.shape
computeCost(X, y, theta)
```

# batch gradient decent（批量梯度下降）

$$
{{\theta }_{j}}:={{\theta }_{j}}-\alpha \frac{\partial }{\partial {{\theta }_{j}}}J\left( \theta  \right)
$$
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)\cdot 1
$$
$$
\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}\left((h_{\theta}(x^{(i)})-y^{(i)})\right)\cdot x^{(i)}
$$

```{python}
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)
    
    for i in range(iters):
        error = (X * theta.T) - y
        
        for j in range(parameters):
            term = np.multiply(error, X[:,j])
            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))
            
        theta = temp
        cost[i] = computeCost(X, y, theta)
        
    return theta, cost
```

```{python}
alpha = 0.01
iters = 1000
```

```{python}
g, cost = gradientDescent(X, y, theta, alpha, iters)
g
cost
```

```{python}
computeCost(X, y, g)
```

```{python}
x = np.linspace(data.Population.min(), data.Population.max(), 100)
f = g[0, 0] + (g[0, 1] * x)

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Traning Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size')
plt.show()
```

```{python}
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('Iterations')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')
plt.show()
```

# 多變量線性回歸
```{python}
path =  'ex1data2.txt'
data2 = pd.read_csv(path, header=None, names=['Size', 'Bedrooms', 'Price'])
data2.head()
data2.describe()
```

```{python}
data2 = (data2 - data2.mean()) / data2.std()
data2.head()
data2.describe()
```

```{python}
# add ones column
data2.insert(0, 'Ones', 1)

# set X (training data) and y (target variable)
cols = data2.shape[1]
X2 = data2.iloc[:,0:cols-1]
y2 = data2.iloc[:,cols-1:cols]

# convert to matrices and initialize theta
X2 = np.matrix(X2.values)
y2 = np.matrix(y2.values)
theta2 = np.matrix(np.array([0,0,0]))

# perform linear regression on the data set
g2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)

# get the cost (error) of the model
computeCost(X2, y2, g2)
```

```{python}
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost2, 'r')
ax.set_xlabel('Iterations')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')
plt.show()
```

```{python}
from sklearn import linear_model
model = linear_model.LinearRegression()
model.fit(X, y)

x = np.array(X[:, 1].A1)
f = model.predict(X).flatten()

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Traning Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size')
plt.show()
```

# normal equation（正規方程）
```{python}
# 正規方程
def normalEqn(X, y):
    theta = np.linalg.inv(X.T@X)@X.T@y# X.T@X等價於X.T.dot(X)
    return theta
```

```{python}
final_theta2=normalEqn(X, y)# 感觉和批量梯度下降的theta的值有点差距
final_theta2
```

