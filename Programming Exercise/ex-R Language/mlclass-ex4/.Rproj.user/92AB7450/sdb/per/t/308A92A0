{
    "collab_server" : "",
    "contents" : "## Machine Learning Online Class - Exercise 4 Neural Network Learning\n\n#  Instructions\n#  ------------\n#\n#  This file contains code that helps you get started on the\n#  linear exercise. You will need to complete the following functions\n#  in this exericse:\n#\n#     sigmoidGradient.R\n#     randInitializeWeights.R\n#     nnCostFunction.R\n#\n#  For this exercise, you will not need to change any code in this file,\n#  or any other files other than those mentioned above.\n#\n\n## Initialization\nrm(list=ls())\nsources <- c(\"checkNNGradients.R\",\"computeNumericalGradient.R\",\n             \"debugInitializeWeights.R\",\"displayData.R\",\"lbfgsb3_.R\",\n             \"nnCostFunction.R\",\"predict.R\",\"randInitializeWeights.R\",\n             \"sigmoid.R\",\"sigmoidGradient.R\")\n\nfor (i in 1:length(sources)) {\n  cat(paste(\"Loading \",sources[i],\"\\n\"))\n  source(sources[i])\n}\n\n## Setup the parameters you will use for this exercise\ninput_layer_size  <- 400  # 20x20 Input Images of Digits\nhidden_layer_size <- 25   # 25 hidden units\nnum_labels <- 10          # 10 labels, from 1 to 10\n                          # (note that we have mapped \"0\" to label 10)\n\n## ------------- Part 1: Loading and Visualizing Data --------------\n#  We start the exercise by first loading and visualizing the dataset.\n#  You will be working with a dataset that contains handwritten digits.\n#\n\n# Load Training Data\ncat(sprintf('Loading and Visualizing Data ...\\n'))\n\nload('ex4data1.Rda')\nlist2env(data,.GlobalEnv)\nrm(data)\n\nm <- dim(X)[1]\n\n# Randomly select 100 data points to display\nsel <- sample(m)\nsel <- sel[1:100]\n\ndisplayData(X[sel,])\n\ncat(sprintf('Program paused. Press enter to continue.\\n'))\nline <- readLines(con = stdin(),1)\n\n\n## ----------------- Part 2: Loading Pameters -----------------\n# In this part of the exercise, we load some pre-initialized\n# neural network parameters.\n\ncat(sprintf('\\nLoading Saved Neural Network Parameters ...\\n'))\n\n# Load the weights into variables Theta1 and Theta2\nload('ex4weights.Rda')\nlist2env(data,.GlobalEnv)\nrm(data)\n\n# Unroll parameters\nnn_params <-c(c(Theta1),c(Theta2))\n\n## ----------------- Part 3: Compute Cost (Feedforward) -----------------\n#  To the neural network, you should first start by implementing the\n#  feedforward part of the neural network that returns the cost only. You\n#  should complete the code in nnCostFunction.R to return cost. After\n#  implementing the feedforward to compute the cost, you can verify that\n#  your implementation is correct by verifying that you get the same cost\n#  as us for the fixed debugging parameters.\n#\n#  We suggest implementing the feedforward cost *without* regularization\n#  first so that it will be easier for you to debug. Later, in part 4, you\n#  will get to implement the regularized cost.\n#\ncat(sprintf('\\nFeedforward Using Neural Network ...\\n'))\n\n# Weight regularization parameter (we set this to 0 here).\nlambda <- 0\n\nJ <- nnCostFunction(input_layer_size, hidden_layer_size,\n                    num_labels, X, y, lambda)(nn_params)\n\ncat(sprintf(('Cost at parameters (loaded from ex4weights): %f \n             \\n(this value should be about 0.287629)\\n'), J))\n\ncat(sprintf('\\nProgram paused. Press enter to continue.\\n'))\nline <- readLines(con = stdin(),1)\n\n## --------------- Part 4: Implement Regularization ---------------\n#  Once your cost function implementation is correct, you should now\n#  continue to implement the regularization with the cost.\n#\n\ncat(sprintf('\\nChecking Cost Function (w/ Regularization) ... \\n'))\n\n# Weight regularization parameter (we set this to 1 here).\nlambda <- 1\n\nJ <- nnCostFunction(input_layer_size, hidden_layer_size,\n                   num_labels, X, y, lambda)(nn_params)\n\ncat(sprintf('Cost at parameters (loaded from ex4weights): %f\n            \\n(this value should be about 0.383770)\\n', J))\n\ncat(sprintf('Program paused. Press enter to continue.\\n'))\nline <- readLines(con = stdin(),1)\n\n\n## ----------------- Part 5: Sigmoid Gradient  -----------------\n#  Before you start implementing the neural network, you will first\n#  implement the gradient for the sigmoid function. You should complete the\n#  code in the sigmoidGradient.R file.\n#\n\ncat(sprintf('\\nEvaluating sigmoid gradient...\\n'))\n\ng <- sigmoidGradient(c(1, -0.5, 0, 0.5, 1))\ncat(sprintf('Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:\\n  '))\ncat(sprintf('%f ', g))\ncat(sprintf('\\n\\n'))\n\ncat(sprintf('Program paused. Press enter to continue.\\n'))\nline <- readLines(con = stdin(),1)\n\n\n## ----------------- Part 6: Initializing Pameters -----------------\n#  In this part of the exercise, you will be starting to implment a three\n#  layer neural network that classifies digits. You will start by\n#  implementing a function to initialize the weights of the neural network\n#  (randInitializeWeights.R)\n\ncat(sprintf('\\nInitializing Neural Network Parameters ...\\n'))\n\ninitial_Theta1 <- randInitializeWeights(input_layer_size, hidden_layer_size)\ninitial_Theta2 <- randInitializeWeights(hidden_layer_size, num_labels)\n\n# Unroll parameters\ninitial_nn_params <- c(initial_Theta1,initial_Theta2)\n\n\n## --------------- Part 7: Implement Backpropagation ---------------\n#  Once your cost matches up with ours, you should proceed to implement the\n#  backpropagation algorithm for the neural network. You should add to the\n#  code you've written in nnCostFunction.R to return the partial\n#  derivatives of the parameters.\n#\ncat(sprintf('\\nChecking Backpropagation... \\n'))\n\n#  Check gradients by running checkNNGradients\ncheckNNGradients()\n\ncat(sprintf('\\nProgram paused. Press enter to continue.\\n'))\nline <- readLines(con = stdin(),1)\n\n\n## --------------- Part 8: Implement Regularization ---------------\n#  Once your backpropagation implementation is correct, you should now\n#  continue to implement the regularization with the cost and gradient.\n#\n\ncat(sprintf('\\nChecking Backpropagation (w/ Regularization) ... \\n'))\n\n#  Check gradients by running checkNNGradients\nlambda <- 3\ncheckNNGradients(lambda)\n\n# Also output the costFunction debugging values\ndebug_J  <- nnCostFunction(input_layer_size,\n                          hidden_layer_size, num_labels, X, y, lambda)(nn_params)\n\ncat(sprintf('\\n\\nCost at (fixed) debugging parameters (w/ lambda <- 10): %f\n(this value should be about 0.576051)\\n\\n', debug_J))\n\ncat(sprintf('Program paused. Press enter to continue.\\n'))\nline <- readLines(con = stdin(),1)\n\n\n## -------------------- Part 8: Training NN --------------------\n#  You have now implemented all the code necessary to train a neural\n#  network. To train your neural network, we will now use \"fmincg\", which\n#  is a function which works similarly to \"fminunc\". Recall that these\n#  advanced optimizers are able to train our cost functions efficiently as\n#  long as we provide them with the gradient computations.\n#\ncat(sprintf('\\nTraining Neural Network... \\n'))\n\n#  You should also try different values of lambda\nlambda <- 1\n\n# Create \"short hand\" for the cost function to be minimized\ncostFunction <- nnCostFunction(input_layer_size, hidden_layer_size, \n                                   num_labels, X, y, lambda) #over nn_params\n\ngradFunction <- nnGradFunction(input_layer_size, hidden_layer_size, \n                               num_labels, X, y, lambda) #over nn_params\n\n# Now, costFunction and gradFunction are functions that take in only one argument (the\n# neural network parameters)\n\n# lbfgsb3 works like fmincg (fast)\nlibrary(lbfgsb3)\n\n# After you have completed the assignment, change the maxit to a larger\n# value to see how more training helps.\nopt <- lbfgsb3_(initial_nn_params, fn= costFunction, gr=gradFunction,\n                  control = list(trace=1,maxit=50))\n\n\nnn_params <- opt$prm\ncost <- opt$f\n\n# Obtain Theta1 and Theta2 back from nn_params\nTheta1 <- matrix(nn_params[1:(hidden_layer_size * (input_layer_size + 1))],\n                 hidden_layer_size, (input_layer_size + 1))\n\nTheta2 <- matrix(nn_params[(1 + (hidden_layer_size * (input_layer_size + 1))):length(nn_params)],\n                 num_labels, (hidden_layer_size + 1))\n\ncat(sprintf('Program paused. Press enter to continue.\\n'))\nline <- readLines(con = stdin(),1)\n\n\n## ------------------- Part 9: Visualize Weights -------------------\n#  You can now \"visualize\" what the neural network is learning by\n#  displaying the hidden units to see what features they are capturing in\n#  the data.\n\ncat(sprintf('\\nVisualizing Neural Network... \\n'))\n\ndisplayData(Theta1[, -1])\n\ncat(sprintf('\\nProgram paused. Press enter to continue.\\n'))\nline <- readLines(con = stdin(),1)\n\n## ------------------- Part 10: Implement Predict -------------------\n#  After training the neural network, we would like to use it to predict\n#  the labels. You will now implement the \"predict\" function to use the\n#  neural network to predict the labels of the training set. This lets\n#  you compute the training set accuracy.\n\npred <- predict(Theta1, Theta2, X)\n\ncat(sprintf('\\nTraining Set Accuracy: %f\\n', mean(pred==y) * 100))\n\ncat('Program paused. Press enter to continue.\\n')\nline <- readLines(con = stdin(),1)\n\n#  To give you an idea of the network's output, you can also run\n#  through the examples one at the a time to see what it is predicting.\n\n#  Randomly permute examples\nrp <- sample(m)\n\nfor (i in 1:m){\n    # Display\n    cat(sprintf('\\nDisplaying Example Image. Press Esc to End\\n'))\n    displayData(X[rp[i], ])\n\n    pred <- predict(Theta1, Theta2, X[rp[i],])\n    cat(sprintf('\\nNeural Network Prediction: %d (y %d) (digit %d)\\n', pred  , y[rp[i]]  ,pred %% 10))\n\n    # line <- readLines(con = stdin(),1)\n    #cat(sprintf('Program paused. Press enter to continue.\\n')\n    #line <- readLines(con = stdin(),1)\n    Sys.sleep(2)\n    #press esc to quit the loop in Rstudio\n}\n",
    "created" : 1509254728852.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "195563674",
    "id" : "308A92A0",
    "lastKnownWriteTime" : 1504906646,
    "last_content_update" : 1504906646,
    "path" : "~/Desktop/OpenCourse/Machine Learning by Standford/machine-learning-course-master/Solutions/mlclass-ex4/ex4.R",
    "project_path" : "ex4.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}