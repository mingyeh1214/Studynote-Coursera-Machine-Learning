}
showdf <- function(df, title = ''){
df %>%
kable(format = 'html', caption = title, digit = 4) %>%
kable_styling(bootstrap_options = c('striped', 'hover'))
}
# input matrix X isn't included 1 column
mapFeature <- function(X, degree = 1){
m <- dim(X)[1]; n <- dim(X)[2]
mf <- rep(1, m)
if(n == 1){
for (i in 1:degree) mf <- cbind(mf, X[, 1] ^ i)
}else if(n == 2){
for(i in 1:degree){
for(j in 0:i) mf <- cbind(mf, X[, 1] ^ (i - j) * X[, 2] ^ j)
}
}
colnames(mf) <- paste('x', 0:(dim(mf)[2] - 1), sep = '')
return(mf)
}
sigmoid <- function(z){
return(1 / (1 + exp(-z)))
}
ggplot() +
stat_function(aes(-10:10), fun = sigmoid) +
ggtitle('Sigmoid Function') +
xlab('x') +
ylab('f(x)')
x <- seq(-10, 10, length = 100)
plot_ly(x = x, y = sigmoid(x),
mode = 'lines', type = 'scatter') %>%
layout(title = 'Sigmoid Function',
xaxis = list(title = 'x', titlefont = f),
yaxis = list(title = 'f(x)', titlefont = f))
sigmoidGradient <- function(z) {
return(sigmoid(z) * (1 - sigmoid(z)))
}
normalEquation <- function(X, y, lambda = 0){
n <- dim(X)[2]
regular <- lambda * diag(1, n)
regular[1, 1] <- 0
theta = solve(t(X) %*% X + regular) %*% t(X) %*% y
rownames(theta) <- paste('theta', 0:(n - 1), sep = '')
return(theta)
}
costFunction <- function(X, y, theta, lambda = 0, type = c('lin', 'log')){
m <- dim(X)[1]
pt <- theta
pt[1] <- 0
A <- lambda / (2 * m) * t(pt) %*% pt
if(type == 'lin') {
return(t(X %*% theta - y) %*% (X %*% theta - y) / (2 * m) + A)
}
else if(type == 'log'){
h <- sigmoid(X %*% theta)
return(1 / m * (-t(y) %*% log(h) - t(1 - y) %*% log(1 - h)) + A)
}
}
gradientFunction <- function(X, y, theta, lambda = 0, type = c('lin', 'log')){
m <- dim(X)[1]
G <- lambda / m * theta
G[1] <- 0
if(type == 'lin') return(1 / m * t(X) %*% (X %*% theta - y) + G)
else if(type == 'log'){
h <- sigmoid(X %*% theta)
return(1 / m * t(X) %*% (h - y) +  G)
}
}
hessian <- function (X, y, theta, lambda = 0) {
m <- dim(X)[1]
n <- dim(X)[2]
L <- lambda / m * diag(n)
L[1, 1] <- 0
h <- sigmoid(X %*% theta)
return(1 / m * t(X) %*% X * diag(h) * diag(1 - h) + L)
}
computeTheta <- function(X, y, theta, alpha = 0, lambda = 0, num_iters,
type = c('lin', 'log')){
n <- length(theta)
theta_vals <- theta
J_vals <- rep(0, num_iters)
if(type == 'lin'){
for (i in 1:num_iters){
theta_vals <- rbind(theta_vals, t(theta))
J_vals[i] <- costFunction(X, y, theta, lambda, type = type)
theta <- theta - alpha * gradientFunction(X, y, theta, lambda, type = type)
}
}else if(type == 'log'){
for (i in 1:num_iters) {
theta_vals <- rbind(theta_vals, t(theta))
J_vals[i] = costFunction(X, y, theta, lambda, type = type)
theta = theta -
solve(hessian(X, y, theta, lambda)) %*%
gradientFunction(X, y, theta, lambda, type = type)
}
}
rownames(theta) <- paste('theta', 0:(n - 1), sep = '')
return(list(theta = theta, theta_vals = theta_vals, J_vals = J_vals))
}
oneVsAll <- function(X, y, lambda = 0, num_iters, num_labels, method = c('matrix', 'optim')){
n <- dim(X)[2]
alpha <- 0
type <- 'log'
for (c in 1:num_labels){
theta_ini <- rep(0, n)
if(method == 'matrix')
theta <- computeTheta(X, (y == c), theta_ini, alpha, lambda, num_iters, type)$theta
else
theta <- optim(theta_ini, X = X, y = (y == c), lambda = lambda, type = type,
fn = costFunction, gr = gradientFunction,
method = "BFGS", control = list(maxit = num_iters))$par
if(c == 1) all_theta <- t(theta)
else all_theta <- rbind(all_theta, t(theta))
}
return(all_theta)
}
predictOneVsAll <- function(all_theta, X){
ps <- sigmoid(X %*% t(all_theta))
return(apply(sigmoid(X %*% t(all_theta)), 1, which.max))
}
displayData <- function(X, example_width = round(sqrt(dim(X)[2])))  {
if (is.vector(X))
X <- t(X)
m <- dim(X)[1]
n <- dim(X)[2]
example_height <- (n / example_width) #20
display_rows <- floor(sqrt(m)) #10
display_cols <- ceiling(m / display_rows) #10
pad <- 1
display_array <-
-matrix(0,pad + display_rows * (example_height + pad),
pad + display_cols * (example_width + pad))
curr_ex <- 1
for (j in 1:display_rows) {
for (i in 1:display_cols) {
if (curr_ex > m)
break
max_val <- max(abs(X[curr_ex,]))
display_array[pad + (j - 1) * (example_height + pad) + (1:example_height),
pad + (i - 1) * (example_width + pad) + (1:example_width)] <-
matrix(X[curr_ex,], example_height, example_width) / max_val
curr_ex <- curr_ex + 1
}
if (curr_ex > m)
break
}
op <- par(bg = "gray")
display_array <- t(apply(display_array,2,rev))
image(
z = display_array,col = gray.colors(100), xaxt = 'n',yaxt = 'n'
)
grid(
nx = display_cols,display_rows,col = 'black',lwd = 2,lty = 1
)
box()
par(op)
}
nnCostFunction  <-
function(input_layer_size, hidden_layer_size, num_labels,X, y, lambda) {
function(nn_params) {
m <- dim(X)[1]
Theta1 <-
matrix(nn_params[1:(hidden_layer_size * (input_layer_size + 1))],
hidden_layer_size, (input_layer_size + 1))
Theta2 <-
matrix(nn_params[(1 + (hidden_layer_size * (input_layer_size + 1))):length(nn_params)],
num_labels, (hidden_layer_size + 1))
I <- diag(num_labels)
Y <- matrix(0, m, num_labels)
for (i in 1:m)
Y[i,] <- I[y[i],]
a1 <- cbind(rep(1,m),X)
z2 <- a1 %*% t(Theta1)
a2 <- cbind(rep(1,dim(z2)[1]), sigmoid(z2))
z3 <- a2 %*% t(Theta2)
a3 <- sigmoid(z3)
h <- a3
p <- sum(Theta1[,-1] ^ 2) + sum(Theta2[,-1] ^ 2)
return(sum((-Y) * log(h) - (1 - Y) * log(1 - h)) / m + lambda * p / (2 * m))
}
}
nnGradFunction  <-
function(input_layer_size, hidden_layer_size, num_labels,
X, y, lambda) {
function(nn_params) {
m <- dim(X)[1]
Theta1 <-
matrix(nn_params[1:(hidden_layer_size * (input_layer_size + 1))],
hidden_layer_size, (input_layer_size + 1))
Theta2 <-
matrix(nn_params[(1 + (hidden_layer_size * (input_layer_size + 1))):length(nn_params)],
num_labels, (hidden_layer_size + 1))
I <- diag(num_labels)
Y <- matrix(0, m, num_labels)
for (i in 1:m)
Y[i,] <- I[y[i],]
a1 <- cbind(rep(1,m),X)
z2 <- a1 %*% t(Theta1)
a2 <- cbind(rep(1,dim(z2)[1]), sigmoid(z2))
z3 <- a2 %*% t(Theta2)
a3 <- sigmoid(z3)
h <- a3
sigma3 <- h - Y
sigma2 <-
(sigma3 %*% Theta2) * sigmoidGradient(cbind(rep(1,dim(z2)[1]),z2))
sigma2 <- sigma2[,-1]
delta_1 <- (t(sigma2) %*% a1)
delta_2 <- (t(sigma3) %*% a2)
p1 <- (lambda / m) * cbind(rep(0,dim(Theta1)[1]), Theta1[,-1])
p2 <- (lambda / m) * cbind(rep(0,dim(Theta2)[1]), Theta2[,-1])
Theta1_grad <- delta_1 / m + p1
Theta2_grad <- delta_2 / m + p2
grad <-  c(c(Theta1_grad), c(Theta2_grad))
return(grad)
}
}
randInitializeWeights <- function(L_in, L_out) {
epsilon_init <- 0.12
rnd <- runif(L_out * (1 + L_in))
rnd <- matrix(rnd,L_out,1 + L_in)
W <- rnd * 2 * epsilon_init - epsilon_init
W
}
predict <- function(Theta1, Theta2, X) {
if (is.vector(X))
X <- t(X)
m <- dim(X)[1]
num_labels <- dim(Theta2)[1]
p <- rep(0,dim(X)[1])
h1 <- sigmoid(cbind(rep(1,m),X) %*% t(Theta1))
h2 <- sigmoid(cbind(rep(1,m),h1) %*% t(Theta2))
p <- apply(h2,1,which.max)
p
}
debugInitializeWeights  <- function(fan_out, fan_in) {
W <- matrix(0,fan_out,1 + fan_in)
W <- matrix(sin(1:length(W)), dim(W)[1],dim(W)[2]) / 10
W
}
computeNumericalGradient <- function(J, theta) {
numgrad <- rep(0,length(theta))
perturb <- rep(0,length(theta))
e <- 1e-4
for (p in 1:length(theta)) {
perturb[p] <- e
loss1 <- J(theta - perturb)
loss2 <- J(theta + perturb)
numgrad[p] <- (loss2 - loss1) / (2 * e)
perturb[p] <- 0
}
numgrad
}
checkNNGradients <- function (lambda = 0) {
input_layer_size <- 3
hidden_layer_size <- 5
num_labels <- 3
m <- 5
Theta1 <- debugInitializeWeights(hidden_layer_size, input_layer_size)
Theta2 <- debugInitializeWeights(num_labels, hidden_layer_size)
X  <- debugInitializeWeights(m, input_layer_size - 1)
y  <- 1 + t(1:m %% num_labels)
nn_params <- c(Theta1,Theta2)
costFunc <- nnCostFunction(input_layer_size, hidden_layer_size,
num_labels, X, y, lambda)
cost <- costFunc(nn_params)
grad <- nnGradFunction(input_layer_size, hidden_layer_size,
num_labels, X, y, lambda)(nn_params)
numgrad <- computeNumericalGradient(costFunc, nn_params)
print(cbind(numgrad, grad))
cat(
sprintf(
'The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)\n\n'
)
)
diff <-
norm(as.matrix(numgrad - grad)) / norm(as.matrix(numgrad + grad))
cat(
sprintf(
'If your backpropagation implementation is correct, then
the relative difference will be small (less than 1e-9).
Relative Difference: %g', diff
)
)
}
lbfgsb3_ <- function (prm, fn, gr = NULL, lower = -Inf, upper = Inf, control = list(),
...)
{
tasklist <- c("NEW_X", "START", "STOP", "FG", "ABNORMAL_TERMINATION_IN_LNSRCH",
"CONVERGENCE", "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL",
"CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH", "ERROR: FTOL .LT. ZERO",
"ERROR: GTOL .LT. ZERO", "ERROR: INITIAL G .GE. ZERO",
"ERROR: INVALID NBD", "ERROR: N .LE. 0", "ERROR: NO FEASIBLE SOLUTION",
"ERROR: STP .GT. STPMAX", "ERROR: STP .LT. STPMIN",
"ERROR: STPMAX .LT. STPMIN", "ERROR: STPMIN .LT. ZERO",
"ERROR: XTOL .LT. ZERO", "FG_LNSRCH", "FG_START", "RESTART_FROM_LNSRCH",
"WARNING: ROUNDING ERRORS PREVENT PROGRESS", "WARNING: STP .eq. STPMAX",
"WARNING: STP .eq. STPMIN", "WARNING: XTOL TEST SATISFIED")
ctrl <- list(maxit = 100, trace = 0, iprint = 0L)
namc <- names(control)
if (!all(namc %in% names(ctrl)))
stop("unknown names in control: ", namc[!(namc %in%
names(ctrl))])
ctrl[namc] <- control
iprint <- as.integer(ctrl$iprint)
factr <- 1e+07
pgtol <- 1e-05
nmax <- 26260
mmax <- 17L
if (length(prm) > nmax)
stop("The number of parameters cannot exceed 1024")
n <- as.integer(length(prm))
m <- 5L
nbd <- rep(2L, n)
nwa <- 2 * mmax * nmax + 5 * nmax + 11 * mmax * mmax + 8 *
mmax
wa <- rep(0, nwa)
dsave <- rep(0, 29)
lsave <- rep(TRUE, 4)
isave <- rep(0L, 44)
iwa <- rep(0L, 3 * nmax)
csave <- ""
if (length(lower) == 1)
lower <- rep(lower, n)
if (length(upper) == 1)
upper <- rep(upper, n)
bigval <- .Machine$double.xmax/10
for (i in 1:n) {
if (is.finite(lower[i])) {
if (is.finite(upper[i]))
nbd[i] <- 2
else {
nbd[i] <- 1
upper[i] <- bigval
}
}
else {
if (is.finite(upper[i])) {
nbd[i] <- 3
lower[i] <- -bigval
}
else {
nbd[i] <- 0
upper[i] <- bigval
lower[i] <- -bigval
}
}
}
itask <- 2L
task <- tasklist[itask]
f <- .Machine$double.xmax/100
g <- rep(f, n)
icsave <- 0
repeat {
if (isave[34] > ctrl$maxit )
break
if (ctrl$trace >= 2) {
cat("Before call, f=", f, "  task number ", itask,
" ")
print(task)
}
possibleError <- tryCatch(
result <- .Fortran("lbfgsb3", n = as.integer(n), m = as.integer(m),
x = as.double(prm), l = as.double(lower), u = as.double(upper),
nbd = as.integer(nbd), f = as.double(f), g = as.double(g),
factr = as.double(factr), pgtol = as.double(pgtol),
wa = as.double(wa), iwa = as.integer(iwa), itask = as.integer(itask),
iprint = as.integer(iprint), icsave = as.integer(icsave),
lsave = as.logical(lsave), isave = as.integer(isave),
dsave = as.double(dsave))
, error = function(e) e)
if(inherits(possibleError, "error"))
break
itask <- result$itask
icsave <- result$icsave
prm <- result$x
g <- result$g
iwa <- result$iwa
wa <- result$wa
nbd <- result$nbd
lsave <- result$lsave
isave <- result$isave
dsave <- result$dsave
if (ctrl$trace > 2) {
cat("returned from lbfgsb3\n")
cat("returned itask is ", itask, "\n")
task <- tasklist[itask]
cat("changed task to ", task, "\n")
}
if (itask %in% c(4L, 20L, 21L)) {
if (ctrl$trace >= 2) {
cat("computing f and g at prm=")
print(prm)
}
f <- fn(prm, ...)
if (is.null(gr)) {
g <- grad(fn, prm, ...)
}
else {
g <- gr(prm, ...)
}
if (ctrl$trace > 0) {
cat("At iteration ", isave[34], " f =", f)
if (ctrl$trace > 1) {
cat("max(abs(g))=", max(abs(g)))
}
cat("\n")
}
}
else {
if (itask == 1L) {
}
else break
}
}
info <- list(task = task, itask = itask, lsave = lsave,
icsave = icsave, dsave = dsave, isave = isave)
ans <- list(prm = prm, f = f, g = g, info = info)
}
path <- "/Users/liumingyeh/Desktop/OpenCourse/Machine Learning by Standford/machine-learning-ex3/ex3/ex3data1.mat"
data <- readMat(path)
X <- cbind(rep(1, dim(data$X)[1]), data$X)
y <- data$y
path <- "/Users/liumingyeh/Desktop/OpenCourse/Machine Learning by Standford/machine-learning-ex3/ex3/ex3weights.mat"
data <- readMat(path)
Theta1 <- data$Theta1
Theta2 <- data$Theta2
dim(X); dim(y); dim(Theta1); dim(Theta2)
z1 <- X %*% t(Theta1)
h1 <- sigmoid(z1)
h1 <- cbind(rep(1, length = dim(h1)[1]), h1)
z2 <- h1 %*% t(Theta2)
h2 <- sigmoid(z2)
pred <- apply(h2, 1, which.max)
accu <- mean(pred == y) * 100
cat("Training Set Accuracy is:", accu, "%\n")
X <- X[, -1]
nn_params <-c(c(Theta1), c(Theta2))
input_layer_size <- dim(X)[2]
hidden_layer_size <- 25
num_labels <- 10
J_vals <- rep(0, 20)
for(i in 1:length(J_vals)){
J_vals[i] <-
nnCostFunction(input_layer_size, hidden_layer_size, num_labels, X, y, lambda = i - 1)(nn_params)
}
ggplot() + geom_line(aes(x = 1:length(J_vals), y = J_vals)) +
labs(x = 'lambda', y = 'J_val')
path <- "/Users/liumingyeh/Desktop/OpenCourse/Machine Learning by Standford/machine-learning-ex3/ex3/ex3data1.mat"
data <- readMat(path)
X <- data$X
y <- data$y
input_layer_size <- 400
hidden_layer_size <- 25
num_labels <- 10
lambda <- 1
initial_Theta1 <- randInitializeWeights(input_layer_size, hidden_layer_size)
initial_Theta2 <- randInitializeWeights(hidden_layer_size, num_labels)
initial_nn_params <- c(initial_Theta1, initial_Theta2)
sum(initial_nn_params)
costFunction <- nnCostFunction(input_layer_size, hidden_layer_size,
num_labels, X, y, lambda)
gradFunction <- nnGradFunction(input_layer_size, hidden_layer_size,
num_labels, X, y, lambda)
# use optim
# nn_params <- optim(par = initial_nn_params,
#                    fn = costFunction, gr = gradFunction,
#                    method = "BFGS", control = list(trace = 1, maxit = 50))$par
# Theta1 <- matrix(nn_params[1:(hidden_layer_size * (input_layer_size + 1))],
#                  hidden_layer_size,
#                  (input_layer_size + 1))
# Theta2 <- matrix(nn_params[(1 + (hidden_layer_size * (input_layer_size + 1))):length(nn_params)],
#                  num_labels,
#                  (hidden_layer_size + 1))
# pred <- predict(Theta1, Theta2, X)
# mean(pred == y) * 100 # 94.16%
# use lbfgsb3
opt <- lbfgsb3_(initial_nn_params,
fn = costFunction, gr = gradFunction,
control = list(trace = 1, maxit = 50))
opt$f
nn_params <- opt$prm
sum(nn_params)
Theta1 <- matrix(nn_params[1:(hidden_layer_size * (input_layer_size + 1))],
hidden_layer_size,
(input_layer_size + 1))
Theta2 <- matrix(nn_params[(1 + (hidden_layer_size * (input_layer_size + 1))):length(nn_params)],
num_labels,
(hidden_layer_size + 1))
pred <- predict(Theta1, Theta2, X)
mean(pred == y) * 100 # 97.1%
# checkNNGradients()
opt <- optim(initial_nn_params,
fn = costFunction, gr = gradFunction,
control = list(trace = 1, maxit = 50))
opt <- optim(initial_nn_params,
fn = costFunction, gr = gradFunction,
method = "BFGS")
opt <- optim(initial_nn_params,
fn = costFunction, gr = gradFunction,
method = "BFGS", control = list(trace = 1, maxit = 5))
opt$f
nn_params <- opt$par
sum(nn_params)
Theta1 <- matrix(nn_params[1:(hidden_layer_size * (input_layer_size + 1))],
hidden_layer_size,
(input_layer_size + 1))
Theta2 <- matrix(nn_params[(1 + (hidden_layer_size * (input_layer_size + 1))):length(nn_params)],
num_labels,
(hidden_layer_size + 1))
pred <- predict(Theta1, Theta2, X)
mean(pred == y) * 100 # 97.1%
